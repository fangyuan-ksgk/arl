{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO on GSM8K with Qwen3-0.6B + vLLM\n",
    "\n",
    "Train a small language model on grade-school math problems using **Group Relative Policy Optimization (GRPO)**.\n",
    "\n",
    "- Base model: `Qwen/Qwen3-0.6B`\n",
    "- Dataset: `openai/gsm8k`\n",
    "- Generation backend: **vLLM**\n",
    "- Framework: HuggingFace **TRL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "# Pin vLLM to a version compatible with TRL 0.28.0 (supports up to 0.12.0)\n",
    "!pip install \"vllm==0.12.0\" --quiet\n",
    "!pip install trl transformers datasets accelerate hf_transfer --quiet\n",
    "\n",
    "# Enable hf_transfer for faster model downloads\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from src.ibrl import IBRLTrainer, IBRLConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7473, Test: 1319\n",
      "Example prompt: [{'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]\n",
      "Gold answer: 72\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and prepare GSM8K dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def extract_gold_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract the numeric answer after ####.\"\"\"\n",
    "    match = re.search(r\"####\\s*(.+)\", answer_text)\n",
    "    if match:\n",
    "        return match.group(1).strip().replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def format_example(example):\n",
    "    \"\"\"Convert to chat-style prompt and attach gold answer.\"\"\"\n",
    "    example[\"prompt\"] = [{\"role\": \"user\", \"content\": example[\"question\"]}]\n",
    "    example[\"gold_answer\"] = extract_gold_answer(example[\"answer\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(format_example)\n",
    "test_dataset = dataset[\"test\"].map(format_example)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "print(f\"Example prompt: {train_dataset[0]['prompt']}\")\n",
    "print(f\"Gold answer: {train_dataset[0]['gold_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness: [1.0]\n",
      "Format: [0.5]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define reward functions\n",
    "\n",
    "def extract_answer_from_completion(text: str) -> str:\n",
    "    \"\"\"Parse the final numeric answer from a model completion.\"\"\"\n",
    "    # Look for #### pattern first\n",
    "    match = re.search(r\"####\\s*([\\d,\\.\\-]+)\", text)\n",
    "    if match:\n",
    "        return match.group(1).strip().replace(\",\", \"\")\n",
    "    # Fallback: last number in the text\n",
    "    numbers = re.findall(r\"-?[\\d,]+\\.?\\d*\", text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(\",\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def correctness_reward(completions: list[list[dict]], gold_answer: list[str], **kwargs) -> list[float]:\n",
    "    \"\"\"Award +1.0 if the model's final numeric answer matches the gold answer.\"\"\"\n",
    "    rewards = []\n",
    "    for completion, gold in zip(completions, gold_answer):\n",
    "        text = completion[0][\"content\"]\n",
    "        predicted = extract_answer_from_completion(text)\n",
    "        try:\n",
    "            correct = float(predicted) == float(gold)\n",
    "        except (ValueError, TypeError):\n",
    "            correct = False\n",
    "        rewards.append(1.0 if correct else 0.0)\n",
    "    return rewards\n",
    "\n",
    "def format_reward(completions: list[list[dict]], **kwargs) -> list[float]:\n",
    "    \"\"\"Award +0.5 if the response contains #### <number> pattern.\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = completion[0][\"content\"]\n",
    "        has_format = bool(re.search(r\"####\\s*[\\d,\\.\\-]+\", text))\n",
    "        rewards.append(0.5 if has_format else 0.0)\n",
    "    return rewards\n",
    "\n",
    "# Quick sanity check\n",
    "test_comp = [[{\"content\": \"The answer is 2+3=5. #### 5\"}]]\n",
    "print(\"Correctness:\", correctness_reward(test_comp, gold_answer=[\"5\"]))\n",
    "print(\"Format:\", format_reward(test_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Configure GRPO / IBRL with vLLM backend\n",
    "config = GRPOConfig(\n",
    "    output_dir=\"grpo_gsm8k_output\",\n",
    "    num_generations=8,               # group size G\n",
    "    max_completion_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=10,\n",
    "    max_steps=20,                    # smoke test; remove for full run\n",
    "    use_vllm=True,\n",
    "    vllm_mode=\"colocate\",           # run vLLM in-process (no separate server needed)\n",
    "    vllm_gpu_memory_utilization=0.3,\n",
    "    bf16=True,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-24 15:21:00] INFO modeling.py:1576: Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 685253632 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b7a84b55e64c41ba12e4a57a4323d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 28.76it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 32.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialize GRPOTrainer / IBRLTrainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    reward_funcs=[correctness_reward, format_reward],\n",
    "    args=config,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/20 00:30 < 00:30, 0.30 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluate — sample generations on test split\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "samples = random.sample(range(len(test_dataset)), 5)\n",
    "for idx in samples:\n",
    "    question = test_dataset[idx][\"question\"]\n",
    "    gold = test_dataset[idx][\"gold_answer\"]\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "    response = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    predicted = extract_answer_from_completion(response)\n",
    "    match = \"CORRECT\" if predicted == gold else \"WRONG\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {question[:120]}...\")\n",
    "    print(f\"A: {response[:300]}\")\n",
    "    print(f\"Predicted: {predicted} | Gold: {gold} | {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plot training reward curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "steps = [entry[\"step\"] for entry in log_history if \"reward\" in entry]\n",
    "rewards = [entry[\"reward\"] for entry in log_history if \"reward\" in entry]\n",
    "\n",
    "if steps:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(steps, rewards, marker=\"o\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.title(\"GRPO Training — Mean Reward over Steps\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No reward data in logs yet (try increasing max_steps or decreasing logging_steps).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
